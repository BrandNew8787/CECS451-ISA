{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pseudocode\n",
        "Q-learning is a popular model-free reinforcement learning algorithm that learns the optimal Q-value function for an environment without requiring a model of the environment's dynamics. The Q-value function estimates the expected cumulative reward that can be obtained by taking a particular action in a particular state and following the optimal policy thereafter.\n",
        "\n",
        "Here is the Q-learning algorithm:\n",
        "\n"
      ],
      "metadata": {
        "id": "e1wr1PziFPrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "1. Initialize Q(s, a) arbitrarily for all state-action pairs\n",
        "2. Repeat the following for each episode:\n",
        "   a. Observe the initial state, s\n",
        "   b. Repeat the following until the episode terminates:\n",
        "      i. Choose an action, a, using an exploration strategy (e.g., epsilon-greedy)\n",
        "      ii. Take action a and observe the resulting reward, r, and the new state, s'\n",
        "      iii. Update the Q-value function using the formula:\n",
        "           Q(s, a) = Q(s, a) + alpha * (r + gamma * max(Q(s', a')) - Q(s, a))\n",
        "           where alpha is the learning rate and gamma is the discount factor\n",
        "      iv. Set the current state to the new state, s = s'\n",
        "   c. Until convergence (i.e., until the Q-value function no longer changes)\n",
        "   ```"
      ],
      "metadata": {
        "id": "T4bOxnUBHKwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Q-learning algorithm gradually learns the optimal Q-value function by repeatedly updating the Q-value estimates based on the observed rewards and the estimated Q-values of the next state-action pairs. The exploration strategy is used to balance exploration and exploitation, and the learning rate determines the degree to which the Q-value function is updated based on new information. The discount factor is used to discount the future rewards, making immediate rewards more valuable than delayed rewards."
      ],
      "metadata": {
        "id": "Bqce-6nYHehv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example environment with 6 states and 2 actions\n",
        "class ExampleEnvironment:\n",
        "    def __init__(self):\n",
        "        self.num_states = 6\n",
        "        self.num_actions = 2\n",
        "        self.state = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.state == 0:\n",
        "            if action == 0:\n",
        "                self.state = 1\n",
        "                reward = 0\n",
        "                done = False\n",
        "            else:\n",
        "                self.state = 0\n",
        "                reward = -1\n",
        "                done = False\n",
        "        elif self.state == 1:\n",
        "            if action == 0:\n",
        "                self.state = 0\n",
        "                reward = -1\n",
        "                done = False\n",
        "            else:\n",
        "                self.state = 2\n",
        "                reward = 0\n",
        "                done = False\n",
        "        elif self.state == 2:\n",
        "            if action == 0:\n",
        "                self.state = 3\n",
        "                reward = 0\n",
        "                done = False\n",
        "            else:\n",
        "                self.state = 1\n",
        "                reward = 0\n",
        "                done = False\n",
        "        elif self.state == 3:\n",
        "            if action == 0:\n",
        "                self.state = 2\n",
        "                reward = 0\n",
        "                done = False\n",
        "            else:\n",
        "                self.state = 4\n",
        "                reward = 0\n",
        "                done = False\n",
        "        elif self.state == 4:\n",
        "            if action == 0:\n",
        "                self.state = 5\n",
        "                reward = 1\n",
        "                done = True\n",
        "            else:\n",
        "                self.state = 3\n",
        "                reward = 0\n",
        "                done = False\n",
        "        elif self.state == 5:\n",
        "            self.state = 5\n",
        "            reward = 0\n",
        "            done = True\n",
        "        return self.state, reward, done, {}\n"
      ],
      "metadata": {
        "id": "Wm17Scqu_uP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, discount, epsilon, num_states, num_actions):\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.discount = discount  # discount factor\n",
        "        self.epsilon = epsilon  # exploration rate\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      #ENTER CODE\n",
        "      pass\n",
        "\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        # Update Q-value for the state-action pair\n",
        "        # ENTER CODE\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "4PsCxVZo_0AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = ExampleEnvironment()\n",
        "    agent = QLearningAgent(alpha=0.5, discount=0.99, epsilon=0.1, num_states=env.num_states,\n",
        "        num_actions=env.num_actions)\n",
        "    num_episodes = 1000\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "          #ENTER CODE\n",
        "          pass\n",
        "    pass\n",
        "\n",
        "    \n",
        "    #print(agent.q_table)\n"
      ],
      "metadata": {
        "id": "Xcj5yvjU_Nqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "column_names = ['action_0', 'action_1']\n",
        "row_names    = ['State_0', 'State_1','State_2','State_3','State_4','State_5']\n",
        "arr_1D = agent.q_table.reshape(-1)\n",
        "arr_2D = arr_1D.reshape(6, 2)\n",
        "df = pd.DataFrame(arr_2D, columns=column_names, index=row_names)\n",
        "df"
      ],
      "metadata": {
        "id": "44m4nHDS62px"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}